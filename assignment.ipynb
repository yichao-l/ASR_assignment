{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR Assignment 2019-20\n",
    "\n",
    "This notebook has been provided as a template to get you started on the assignment.  Feel free to use it for your development, or do your development directly in Python.\n",
    "\n",
    "You can find a full description of the assignment [here](http://www.inf.ed.ac.uk/teaching/courses/asr/2019-20/coursework.pdf).\n",
    "\n",
    "You are provided with two Python modules `observation_model.py` and `wer.py`.  The first was described in [Lab 3](https://github.com/Ore-an/asr_lab3/blob/master/asr_lab3.ipynb).  The second can be used to compute the number of substitution, deletion and insertion errors between ASR output and a reference text.\n",
    "\n",
    "It can be used as follows:\n",
    "\n",
    "```python\n",
    "import wer\n",
    "\n",
    "my_refence = 'A B C'\n",
    "my_output = 'A C C D'\n",
    "\n",
    "wer.compute_alignment_errors(my_reference, my_output)\n",
    "```\n",
    "\n",
    "This produces a tuple $(s,d,i)$ giving counts of substitution,\n",
    "deletion and insertion errors respectively - in this example (1, 0, 1).  The function accepts either two strings, as in the example above, or two lists.  Matching is case sensitive.\n",
    "\n",
    "## Template code\n",
    "\n",
    "Assuming that you have already made a function to generate an WFST, `create_wfst()` and a decoder class, `MyViterbiDecoder`, you can perform recognition on all the audio files as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ce16ec4a9588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mobservation_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopenfst_python\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/asr_labs/asr_assignment/observation_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mnnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/asr_labs/asr_assignment/observation_model.py\u001b[0m in \u001b[0;36minitialize_nn\u001b[0;34m(debug)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minitialize_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     nnet = pexpect.spawnu(\"{}/lab-chain-compute-post\".format(self.bindir),\n\u001b[0m\u001b[1;32m     10\u001b[0m                           [\"--feature-type=fbank\",\n\u001b[1;32m     11\u001b[0m                            \u001b[0;34m\"--frame-subsampling-factor=1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import wer\n",
    "import observation_model\n",
    "import openfst_python as fst\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e476878ca0e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# call these two functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mlex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lexicon.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mword_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphone_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_symbol_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-e476878ca0e4>\u001b[0m in \u001b[0;36mgenerate_symbol_tables\u001b[0;34m(lexicon, n)\u001b[0m\n\u001b[1;32m     35\u001b[0m     '''\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mstate_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSymbolTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mphone_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSymbolTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mword_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSymbolTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fst' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def parse_lexicon(lex_file):\n",
    "    \"\"\"\n",
    "    Parse the lexicon file and return it in dictionary form.\n",
    "    \n",
    "    Args:\n",
    "        lex_file (str): filename of lexicon file with structure '<word> <phone1> <phone2>...'\n",
    "                        eg. peppers p eh p er z\n",
    "\n",
    "    Returns:\n",
    "        lex (dict): dictionary mapping words to list of phones\n",
    "    \"\"\"\n",
    "    \n",
    "    lex = {}  # create a dictionary for the lexicon entries (this could be a problem with larger lexica)\n",
    "    with open(lex_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.split()  # split at each space\n",
    "            lex[line[0]] = line[1:]  # first field the word, the rest is the phones\n",
    "    return lex\n",
    "\n",
    "def generate_symbol_tables(lexicon, n=3):\n",
    "    '''\n",
    "    Return word, phone and state symbol tables based on the supplied lexicon\n",
    "        \n",
    "    Args:\n",
    "        lexicon (dict): lexicon to use, created from the parse_lexicon() function\n",
    "        n (int): number of states for each phone HMM\n",
    "        \n",
    "    Returns:\n",
    "        word_table (fst.SymbolTable): table of words\n",
    "        phone_table (fst.SymbolTable): table of phones\n",
    "        state_table (fst.SymbolTable): table of HMM phone-state IDs\n",
    "    '''\n",
    "    \n",
    "    state_table = fst.SymbolTable()\n",
    "    phone_table = fst.SymbolTable()\n",
    "    word_table = fst.SymbolTable()\n",
    "    \n",
    "    # add empty <eps> symbol to all tables\n",
    "    state_table.add_symbol('<eps>')\n",
    "    phone_table.add_symbol('<eps>')\n",
    "    word_table.add_symbol('<eps>')\n",
    "    \n",
    "    for word, phones  in lexicon.items():\n",
    "        \n",
    "        word_table.add_symbol(word)\n",
    "        \n",
    "        for p in phones: # for each phone\n",
    "            \n",
    "            phone_table.add_symbol(p)\n",
    "            for i in range(1,n+1): # for each state 1 to n\n",
    "                state_table.add_symbol('{}_{}'.format(p, i))\n",
    "            \n",
    "    return word_table, phone_table, state_table\n",
    "\n",
    "\n",
    "# call these two functions\n",
    "lex = parse_lexicon('lexicon.txt')\n",
    "word_table, phone_table, state_table = generate_symbol_tables(lex)\n",
    "\n",
    "\n",
    "def generate_phone_wfst(f, start_state, phone, n):\n",
    "    \"\"\"\n",
    "    Generate a WFST representating an n-state left-to-right phone HMM\n",
    "    \n",
    "    Args:\n",
    "        f (fst.Fst()): an FST object, assumed to exist already\n",
    "        start_state (int): the index of the first state, assmed to exist already\n",
    "        phone (str): the phone label \n",
    "        n (int): number of states for each phone HMM\n",
    "        \n",
    "    Returns:\n",
    "        the final state of the FST\n",
    "    \"\"\"\n",
    "    \n",
    "    current_state = start_state\n",
    "    \n",
    "    for i in range(1, n+1):\n",
    "        \n",
    "        in_label = state_table.find('{}_{}'.format(phone, i))\n",
    "        \n",
    "        sl_weight = fst.Weight('log', -math.log(0.1))  # weight for self-loop\n",
    "        # self-loop back to current state\n",
    "        f.add_arc(current_state, fst.Arc(in_label, 0, sl_weight, current_state))\n",
    "        \n",
    "        # transition to next state\n",
    "        \n",
    "        # we want to output the phone label on the final state\n",
    "        # note: if outputting words instead this code should be modified\n",
    "        if i == n:\n",
    "            out_label = phone_table.find(phone)\n",
    "        else:\n",
    "            out_label = 0   # output empty <eps> label\n",
    "            \n",
    "        next_state = f.add_state()\n",
    "        next_weight = fst.Weight('log', -math.log(0.9)) # weight to next state\n",
    "        f.add_arc(current_state, fst.Arc(in_label, out_label, next_weight, next_state))    \n",
    "       \n",
    "        current_state = next_state\n",
    "        \n",
    "    return current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_wfst(word):\n",
    "    \"\"\" Generate a WFST for any word in the lexicon, composed of 3-state phone WFSTs.\n",
    "        This will currently output word labels.  \n",
    "        Exercise: could you modify this function and the one above to output a single phone label instead?\n",
    "    \n",
    "    Args:\n",
    "        word (str): the word to generate\n",
    "        \n",
    "    Returns:\n",
    "        the constructed WFST\n",
    "    \n",
    "    \"\"\"\n",
    "    f = fst.Fst('log')\n",
    "    \n",
    "    # create the start state\n",
    "    start_state = f.add_state()\n",
    "    f.set_start(start_state)\n",
    "    \n",
    "    current_state = start_state\n",
    "    \n",
    "    # iterate over all the phones in the word\n",
    "    for phone in lex[word]:   # will raise an exception if word is not in the lexicon\n",
    "        \n",
    "        current_state = generate_phone_wfst(f, current_state, phone, 3)\n",
    "    \n",
    "        # note: new current_state is now set to the final state of the previous phone WFST\n",
    "        \n",
    "    f.set_final(current_state)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_words_wfst(word_list):\n",
    "    \"\"\" Generate a WFST for any word in the lexicon, composed of 3-state phone WFSTs.\n",
    "        This will currently output word labels.  \n",
    "        Exercise: could you modify this function and the one above to output a single phone label instead?\n",
    "    \n",
    "    Args:\n",
    "        word (str): the word to generate\n",
    "        \n",
    "    Returns:\n",
    "        the constructed WFST\n",
    "    \n",
    "    \"\"\"\n",
    "    if isinstance(word_list, str):\n",
    "        word_list = word_list.split()\n",
    "    f = fst.Fst(\"log\")\n",
    "    start_state = f.add_state()\n",
    "    f.set_start(start_state)\n",
    "    for word in word_list:\n",
    "        # create the start state\n",
    "        \n",
    "        current_state = f.add_state()\n",
    "        \n",
    "        f.add_arc(start_state, fst.Arc(0, 0, fst.Weight(\"log\",-math.log(1/len(word_list))), current_state))\n",
    "    \n",
    "        # iterate over all the phones in the word\n",
    "        for phone in lex[word]:   # will raise an exception if word is not in the lexicon\n",
    "        \n",
    "            current_state = generate_phone_wfst(f, current_state, phone, 3)\n",
    "            \n",
    "    \n",
    "            # note: new current_state is now set to the final state of the previous phone WFST\n",
    "        \n",
    "        f.add_arc(current_state, fst.Arc(0, 0, fst.Weight(\"log\",-math.log(1)), start_state))\n",
    "    \n",
    "        f.set_final(current_state)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViterbiDecoder:\n",
    "    \n",
    "    NLL_ZERO = 1e10  # define a constant representing -log(0).  This is really infinite, but approximate\n",
    "                     # it here with a very large number\n",
    "    \n",
    "    def __init__(self, f, audio_file_name):\n",
    "        \"\"\"Set up the decoder class with an audio file and WFST f\n",
    "        \"\"\"\n",
    "        self.om = observation_model.ObservationModel()\n",
    "        self.f = f\n",
    "        \n",
    "        # forward computation counter\n",
    "        self.forward_counter = 0\n",
    "        \n",
    "        if audio_file_name:\n",
    "            self.om.load_audio(audio_file_name)\n",
    "        else:\n",
    "            self.om.load_dummy_audio()\n",
    "        \n",
    "        self.initialise_decoding()\n",
    "    \n",
    "    def initialise_decoding(self):\n",
    "        \"\"\"set up the values for V_j(0) (as negative log-likelihoods)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.V = []\n",
    "        self.backtracing = []\n",
    "        for t in range(self.om.observation_length()+1):\n",
    "            self.V.append([self.NLL_ZERO]*self.f.num_states())\n",
    "        for t in range(self.om.observation_length()+1):\n",
    "            self.backtracing.append([self.NLL_ZERO]*self.f.num_states())\n",
    "        #self.backtracing = [[self.NLL_ZERO]*(self.om.observation_length()+1)]*self.f.num_states()\n",
    "        \n",
    "        # The above code means that self.V[t][j] for t = 0, ... T gives the Viterbi cost\n",
    "        # of state j, time t (in negative log-likelihood form)\n",
    "        # Initialising the costs to NLL_ZERO effectively means zero probability    \n",
    "        \n",
    "        # give the WFST start state a probability of 1.0   (NLL = 0.0)\n",
    "        self.V[0][f.start()] = 0.0\n",
    "        \n",
    "        # some WFSTs might have arcs with epsilon on the input (you might have already created \n",
    "        # examples of these in earlier labs) these correspond to non-emitting states, \n",
    "        # which means that we need to process them without stepping forward in time.  \n",
    "        # Don't worry too much about this!  \n",
    "        self.traverse_epsilon_arcs(0)\n",
    "        \n",
    "    def traverse_epsilon_arcs(self, t):\n",
    "        \"\"\"Traverse arcs with <eps> on the input at time t\n",
    "        \n",
    "        These correspond to transitions that don't emit an observation\n",
    "        \n",
    "        We've implemented this function for you as it's slightly trickier than\n",
    "        the normal case.  You might like to look at it to see what's going on, but\n",
    "        don't worry if you can't fully follow it.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        states_to_traverse = list(range(self.f.num_states())) # traverse all states\n",
    "        while states_to_traverse:\n",
    "            \n",
    "            # Set i to the ID of the current state, the first \n",
    "            # item in the list (and remove it from the list)\n",
    "            i = states_to_traverse.pop(0)   \n",
    "        \n",
    "            # don't bother traversing states which have zero probability\n",
    "            if self.V[t][i] == self.NLL_ZERO:\n",
    "                    continue\n",
    "        \n",
    "            for arc in self.f.arcs(i):\n",
    "                \n",
    "                if arc.ilabel == 0:     # if <eps> transition\n",
    "                  \n",
    "                    j = arc.nextstate   # ID of next state  \n",
    "                \n",
    "                    if self.V[t][j] > self.V[t][i] + float(arc.weight):\n",
    "                        \n",
    "                        # this means we've found a lower-cost path to\n",
    "                        # state j at time t.  We might need to add it\n",
    "                        # back to the processing queue.\n",
    "                        self.V[t][j] = self.V[t][i] + float(arc.weight)\n",
    "                  \n",
    "                        if j not in states_to_traverse:\n",
    "                            states_to_traverse.append(j)\n",
    "\n",
    "    \n",
    "    def forward_step(self, t):\n",
    "        \n",
    "        \n",
    "        states_to_traverse = list(range(self.f.num_states())) # traverse all states\n",
    "        while states_to_traverse:\n",
    "            \n",
    "            # Set i to the ID of the current state, the first \n",
    "            # item in the list (and remove it from the list)\n",
    "            i = states_to_traverse.pop(0)\n",
    "\n",
    "            for arc in self.f.arcs(i):\n",
    "                \n",
    "                # increase the forward counter by 1\n",
    "                self.forward_counter += 1\n",
    "                \n",
    "                if arc.ilabel != 0:\n",
    "                    \n",
    "                    j = arc.nextstate   # ID of next state  \n",
    "                \n",
    "                    if self.V[t][j] > self.V[t-1][i] + float(arc.weight) - self.om.log_observation_probability(state_table.find(arc.ilabel),t):\n",
    "                        \n",
    "                        # this means we've found a lower-cost path to\n",
    "                        # state j at time t.  We might need to add it\n",
    "                        # back to the processing queue.\n",
    "                        self.V[t][j] = self.V[t-1][i] + float(arc.weight) - self.om.log_observation_probability(state_table.find(arc.ilabel),t)\n",
    "                        self.backtracing[t][j] = (i,state_table.find(arc.ilabel),phone_table.find(arc.olabel))\n",
    "                else:\n",
    "                    self.traverse_epsilon_arcs(t)\n",
    "                \n",
    "\n",
    "    \n",
    "    def finalise_decoding(self):\n",
    "        self.P = self.NLL_ZERO\n",
    "        states_to_transverse = list(range(self.f.num_states()))\n",
    "        while states_to_transverse:\n",
    "            \n",
    "            i = states_to_transverse.pop(0)\n",
    "            \n",
    "            for arc in self.f.arcs(i):\n",
    "                if arc.ilabel != 0:\n",
    "                    j = arc.nextstate   # ID of next state  \n",
    "                    \n",
    "                \n",
    "                    if self.P > self.V[self.om.observation_length()-1][i]+float(arc.weight) - self.om.log_observation_probability(state_table.find(arc.ilabel),self.om.observation_length()-1):\n",
    "                        \n",
    "                        # this means we've found a lower-cost path to\n",
    "                        # state j at time t.  We might need to add it\n",
    "                        # back to the processing queue.\n",
    "                        self.P = self.V[self.om.observation_length()-1][i]+float(arc.weight) - self.om.log_observation_probability(state_table.find(arc.ilabel),self.om.observation_length()-1)\n",
    "                        self.fin_arg = i\n",
    "                        self.last_output = phone_table.find(arc.olabel)\n",
    "                  \n",
    "            \n",
    "    \n",
    "    def decode(self):\n",
    "        \n",
    "        self.initialise_decoding()\n",
    "        t = 1\n",
    "        while t <= self.om.observation_length():\n",
    "            self.forward_step(t)\n",
    "            t+=1\n",
    "        \n",
    "        self.finalise_decoding()\n",
    "    \n",
    "    def backtrace(self):\n",
    "        \n",
    "        # TODO - exercise \n",
    "        \n",
    "        # complete code to trace back through the\n",
    "        # best state sequence\n",
    "        \n",
    "        # You'll need to create a structure B_j(t) to store the \n",
    "        # back-pointers (see lectures), and amend the functions above to fill it.\n",
    "        #best_state_sequence = self.backtracing[self.fin_arg]\n",
    "        best_last_state = self.backtracing[-1][self.fin_arg][0]\n",
    "        # print(best_last_state)\n",
    "        best_state_sequence = []\n",
    "        output_word = [self.last_output]\n",
    "        for t in range(1,self.om.observation_length()+1):\n",
    "            best_state_sequence.append(self.backtracing[-t][best_last_state][1])\n",
    "            output_word.append(self.backtracing[-t][best_last_state][2])\n",
    "            best_last_state = self.backtracing[-t][best_last_state][0]\n",
    "            \n",
    "         #   print(best_last_state)\n",
    "        best_state_sequence = best_state_sequence[::-1]\n",
    "        output_word = [v for v in output_word[::-1] if v!='<eps>']\n",
    "        return best_state_sequence, output_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wfst():\n",
    "    f = generate_multiple_words_wfst([k for k in lex.keys()])\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_multiple_words_wfst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2ba853cd1b50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_wfst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_input_symbols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_output_symbols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphone_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-ab26cf759d2f>\u001b[0m in \u001b[0;36mcreate_wfst\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_wfst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_multiple_words_wfst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_multiple_words_wfst' is not defined"
     ]
    }
   ],
   "source": [
    "f = create_wfst()\n",
    "f.set_input_symbols(state_table)\n",
    "f.set_output_symbols(phone_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_multiple_words_wfst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c9b125c3a743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_wfst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyViterbiDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-ab26cf759d2f>\u001b[0m in \u001b[0;36mcreate_wfst\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_wfst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_multiple_words_wfst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_multiple_words_wfst' is not defined"
     ]
    }
   ],
   "source": [
    "def read_transcription(wav_file):\n",
    "    \"\"\"\n",
    "    Get the transcription corresponding to wav_file.\n",
    "    \"\"\"\n",
    "    \n",
    "    transcription_file = os.path.splitext(wav_file)[0] + '.txt'\n",
    "    \n",
    "    with open(transcription_file, 'r') as f:\n",
    "        transcription = f.readline().strip()\n",
    "    \n",
    "    return transcription\n",
    "\n",
    "\n",
    "f = create_wfst()\n",
    "\n",
    "\n",
    "for wav_file in glob.glob('/group/teaching/asr/labs/recordings/*.wav'):\n",
    "    \n",
    "    decoder = MyViterbiDecoder(f, wav_file)\n",
    "    \n",
    "    decoder.decode()\n",
    "    (state_path, words) = decoder.backtrace()  # you'll need to modify the backtrace() from Lab 4\n",
    "                                               # to return the words along the best path\n",
    "    \n",
    "    transcription = read_transcription(wav_file)\n",
    "    error_counts = wer.compute_alignment_errors(transcription, words)\n",
    "    word_count = len(transcription.split())\n",
    "    \n",
    "    print(error_counts, word_count)     # you'll need to accumulate these to produce an overall Word Error Rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Initial systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob.glob('/group/teaching/asr/labs/recordings/*.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function states:\n",
      "\n",
      "states(...) method of pywrapfst._MutableFst instance\n",
      "    states(self)\n",
      "    \n",
      "    Returns an iterator over all states in the FST.\n",
      "    \n",
      "    Returns:\n",
      "      A StateIterator object for the FST.\n",
      "    \n",
      "    See also: `arcs`, `mutable_arcs`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(f.states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 213)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_states_arcs (f):\n",
    "    '''\n",
    "    Count the number of states and arcs in an wfst\n",
    "    type f pywrapfst\n",
    "    para f the wfst to count the number of states and arcs for\n",
    "    '''\n",
    "    num_states = 0\n",
    "    num_arcs = 0\n",
    "    \n",
    "    for state in f.states():\n",
    "        num_states += 1\n",
    "\n",
    "        for arc in f.arcs(state):\n",
    "            num_arcs += 1\n",
    "\n",
    "            \n",
    "    return num_states,num_arcs\n",
    "            \n",
    "count_state_arcs(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp():\n",
    "    '''\n",
    "    Run a test on the test data, record the WER, speed and memory cost.\n",
    "    '''\n",
    "    f = create_wfst()\n",
    "    decoder = MyViterbiDecoder(f, '')\n",
    "    \n",
    "    # store the error counts and word counts\n",
    "    tot_errors,tot_words = 0,0\n",
    "    \n",
    "    for wav_file in glob.glob('/group/teaching/asr/labs/recordings/*.wav'):\n",
    "    \n",
    "        decoder.om.load_audio(wav_file)\n",
    "        \n",
    "        # start a timer\n",
    "        start = time.time()\n",
    "        \n",
    "        decoder.decode()\n",
    "        (state_path, words) = decoder.backtrace()  # you'll need to modify the backtrace() from Lab 4\n",
    "                                                   # to return the words along the best path\n",
    "        # stop the timer\n",
    "        end = time.time()\n",
    "        time_cost = end - start\n",
    "        \n",
    "        # save the forward computation counter\n",
    "        computation_counter = decoder.forward_counter\n",
    "        \n",
    "        # save the number states and arcs\n",
    "        num_states,num_arcs = count_states_arcs(f)\n",
    "        \n",
    "        transcription = read_transcription(wav_file)\n",
    "        error_counts = wer.compute_alignment_errors(transcription, words)\n",
    "        word_count = len(transcription.split())\n",
    "        \n",
    "        # increase the total error and word count\n",
    "        tot_errors += error_counts\n",
    "        tot_words += word_count\n",
    "\n",
    "        print(\"\"\"\n",
    "        Run time: {}, \n",
    "        Number of forward computations: {},\n",
    "        Number of states and arcs: {} {},\n",
    "        Number of errors {} in {} words\n",
    "        \"\"\".format(time_cost,num_states,num_arcs,tot_errors,tot_words))     # you'll need to accumulate these to produce an overall Word Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hi\n",
      "mu23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a=23\n",
    "print(\"\"\"\n",
    "hi\n",
    "mu{}\n",
    "\"\"\".format(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall WER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed (time/ the number of forward computations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### memory (the number of states and arcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2 - System tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task3 - Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task4 - Advanced topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
